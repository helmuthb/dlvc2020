{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Style\n",
    "\n",
    "This is a style transfer, from one image (e.g. an oil painting or similar) to\n",
    "another image (e.g. a photograph).\n",
    "\n",
    "The code is almost literally taken from https://github.com/DOsinga/deep_learning_cookbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import vgg16, vgg19\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import imageio\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from itertools import combinations\n",
    "\n",
    "from imageio import imread, imsave\n",
    "from skimage.transform import resize\n",
    "\n",
    "try:\n",
    "    from io import BytesIO\n",
    "except ImportError:\n",
    "    from StringIO import StringIO as BytesIO\n",
    "import PIL\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Routines\n",
    "\n",
    "Some helper routines to pre-process an image, and to show a pre-processed image again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(a, fmt='jpeg'):\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "\n",
    "def preprocess_image(image_path, target_size=None):\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x, w, h):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, w, h))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((w, h, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_path = 'style_transfer/Okerk2.jpg'\n",
    "style_image_path = 'style_transfer/VanGogh-starry_night_ballance1.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = load_img(base_image_path).size\n",
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_image = K.variable(preprocess_image(style_image_path, target_size=(h, w)))\n",
    "combination_image = K.placeholder(style_image.shape)\n",
    "input_tensor = K.concatenate([base_image,\n",
    "                              style_image,\n",
    "                              combination_image], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained classifier\n",
    "\n",
    "We use a pretrained model as the \"classifier\" for the style.\n",
    "In addition we already specify the input tensor (our style image) - we will\n",
    "optimize the result to have a similar gram matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16.VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Methods to define the loss and the gradients for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    def __init__(self, loss_total, result_image, **other):\n",
    "        grads = K.gradients(loss_total, result_image)\n",
    "        outputs = [loss_total] + list(other.values()) + grads\n",
    "        self.iterate = K.function([result_image], outputs)\n",
    "        self.other = list(other.keys())\n",
    "        self.other_values = {}\n",
    "        self.shape = result_image.shape\n",
    "\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        outs = self.iterate([x.reshape(self.shape)])\n",
    "        self.loss_value = outs[0]\n",
    "        self.grad_values = outs[-1].flatten().astype('float64')\n",
    "        self.other_values = dict(zip(self.other, outs[1:-1]))\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        return np.copy(self.grad_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Loop\n",
    "\n",
    "Our optimization is using the `fmin_l_bfgs_b` implementation from `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(evaluator, image, num_iter=25):\n",
    "    for i in range(num_iter):\n",
    "        start_time = time.time()\n",
    "\n",
    "        image, min_val, info = fmin_l_bfgs_b(evaluator.loss, image.flatten(), fprime=evaluator.grads, maxfun=20)\n",
    "\n",
    "        end_time = time.time()\n",
    "        clear_output()\n",
    "        showarray(deprocess_image(image.copy(), h, w))\n",
    "\n",
    "        print(\"Iteration %d completed in %ds\" % (i + 1, end_time - start_time))\n",
    "        print(\"Current loss value:\", min_val)\n",
    "        print(' '.join(k + ':' + str(evaluator.other_values[k]) for k in evaluator.other))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x, exp=1.25):\n",
    "    _, d1, d2, d3 = x.shape\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        a = K.square(x[:, :, :d2 - 1, :d3 - 1] - x[:, :, 1:, :d3 - 1])\n",
    "        b = K.square(x[:, :, :d2 - 1, :d3 - 1] - x[:, :, :d2 - 1, 1:])\n",
    "    else:\n",
    "        a = K.square(x[:, :d1 - 1, :d2 - 1, :] - x[:, 1:, :d2 - 1, :])\n",
    "        b = K.square(x[:, :d1 - 1, :d2 - 1, :] - x[:, :d1 - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    if K.image_data_format() != 'channels_first':\n",
    "        x = K.permute_dimensions(x, (2, 0, 1))\n",
    "    features = K.batch_flatten(x)\n",
    "    return K.dot(features - 1, K.transpose(features - 1)) - 1\n",
    "\n",
    "def style_loss(layer_1, layer_2):\n",
    "    gr1 = gram_matrix(layer_1)\n",
    "    gr2 = gram_matrix(layer_2)\n",
    "    return K.sum(K.square(gr1 - gr2)) / (np.prod(layer_2.shape) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_outputs = [layer.output for layer in model.layers if '_conv' in layer.name]\n",
    "\n",
    "loss_content = content_loss(feature_outputs[-1][0, :, :, :],\n",
    "                            feature_outputs[-1][2, :, :, :])\n",
    "loss_variation = total_variation_loss(combination_image)\n",
    "loss_style = K.variable(0.)\n",
    "for idx, layer_features in enumerate(feature_outputs):\n",
    "    loss_style = loss_style + style_loss(layer_features[1, :, :, :], layer_features[2, :, :, :]) * (0.5 ** idx)\n",
    "\n",
    "loss_content /= 40\n",
    "loss_variation /= 10000\n",
    "\n",
    "loss_total = loss_content + loss_variation + loss_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_evaluator = Evaluator(loss_total, combination_image, loss_content=loss_content, \n",
    "                               loss_variation=loss_variation, loss_style=loss_style)\n",
    "run(combined_evaluator, preprocess_image(base_image_path), num_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
