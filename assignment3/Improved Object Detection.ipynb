{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Object Detection\n",
    "\n",
    "This is a slightly improved version of the naive object detection approach.\n",
    "Here the sliding window is run over the output of the last (non-dense layer)\n",
    "of vgg16 (called `block5_pool`).\n",
    "This is also called the _bottom_ model, and it can be retrieved\n",
    "by using `include_top=False` when loading the model from\n",
    "the `vgg16` module.\n",
    "\n",
    "One other effect is that then the model is independent of the input size.\n",
    "By choosing a double as large image, the output of the last layer of the\n",
    "bottom model just gets twice as large as well.\n",
    "\n",
    "Our approach is therefore:\n",
    "\n",
    "* Separate bottom and top model\n",
    "* Run the (double-size) image through the bottom model\n",
    "* The output (14x14x512) will then be split into regions of 7x7x512 (sliding window)\n",
    "* These regions will be fed into the top model\n",
    "\n",
    "The output is essentially the same - but we are running the big picture\n",
    "only once through the bottom model, and only the top model (having just a couple\n",
    "of layers) is run for each regio.\n",
    "\n",
    "The code is almost literally taken from https://github.com/DOsinga/deep_learning_cookbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Flatten, Dense, Input, TimeDistributed\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from keras.preprocessing import image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from scipy.misc import imread, imresize, imsave, fromimage, toimage\n",
    "\n",
    "try:\n",
    "    from io import BytesIO\n",
    "except ImportError:\n",
    "    from StringIO import StringIO as BytesIO\n",
    "import PIL\n",
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Routines\n",
    "\n",
    "Some helper routines to pre-process an image, and to show a pre-processed image again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(a, fmt='jpeg'):\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "\n",
    "def preprocess_image(image_path, target_size=None):\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x, w, h):\n",
    "    x = x.copy()\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, w, h))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((w, h, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Top Model\n",
    "\n",
    "For getting the _top_ model we have to recreate its layers manually, and then copy over the trained\n",
    "weights from `vgg16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_model(base_model):\n",
    "    inputs = Input(shape=(7, 7, 512), name='input')\n",
    "    flatten = Flatten(name='flatten')(inputs)\n",
    "    fc1 = Dense(4096, activation='relu', name='fc1')(flatten)\n",
    "    fc2 = Dense(4096, activation='relu', name='fc2')(fc1)\n",
    "    predictions = Dense(1000, activation='softmax', name='predictions')(fc2)\n",
    "    model = Model(inputs, predictions, name='top_model')\n",
    "    for layer in model.layers:\n",
    "        if layer.name != 'input':\n",
    "            print(layer.name)\n",
    "            layer.set_weights(base_model.get_layer(layer.name).get_weights())\n",
    "    return model\n",
    "\n",
    "top_model = create_top_model(base_model)\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Bottom Model\n",
    "\n",
    "For getting the _bottom_ model we just call `vgg16.VGG16` but this time with `include_top=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "bottom_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading image\n",
    "\n",
    "Loading an image, preprocess it, and output the preprocessed image for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dog2 = preprocess_image('data/cat_dog.jpg', target_size=(448, 448))\n",
    "showarray(deprocess_image(cat_dog2, 448, 448))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Bottom Model\n",
    "\n",
    "We now run the bottom model on the whole image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_out = bottom_model.predict(cat_dog2)\n",
    "bottom_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating regions\n",
    "\n",
    "Using a sliding 7x7 window, create 49 regions which will then be run throgh the top model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = []\n",
    "rects = []\n",
    "for x in range(7):\n",
    "    for y in range(7):\n",
    "        crops.append(bottom_out[0, x: x + 7, y: y + 7, :])\n",
    "        rects.append((y * 32, x * 32, 224 + y * 32, 224 + x * 32))\n",
    "crops = np.asarray(crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Top Model on Regions\n",
    "\n",
    "And show top results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = top_model.predict(crops)\n",
    "crop_scores = defaultdict(list)\n",
    "for idx, pred in enumerate(vgg16.decode_predictions(preds, top=1)):\n",
    "    _, label, weight = pred[0]\n",
    "    crop_scores[label].append((idx, weight))\n",
    "crop_scores.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show top results\n",
    "\n",
    "Using manually selected classes, show the top regions for the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_best_region_for_label(l, draw, label, color=(0,0,0)):\n",
    "    idx = max(l[label], key=lambda t:t[1])[0]\n",
    "    draw.rectangle(rects[idx], outline=color)\n",
    "    \n",
    "cat_dog_img = image.load_img('data/cat_dog.jpg', target_size=(448, 448))\n",
    "draw = ImageDraw.Draw(cat_dog_img)\n",
    "draw_best_region_for_label(crop_scores, draw, 'tabby', (255,0,0))\n",
    "draw_best_region_for_label(crop_scores, draw, 'golden_retriever', (0,255,0))\n",
    "cat_dog_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
