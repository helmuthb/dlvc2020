% !TeX spellcheck = en_US
\documentclass[sigconf,nonacm]{acmart}
\usepackage{listings}
\settopmatter{printacmref=false}
\pagestyle{empty}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
\copyrightyear{2020}
\acmYear{2020}
\setcopyright{rightsretained}
\begin{document}
\title{Deep Learning for Visual Computing}
\subtitle{Group 12, Exercise 2: Iterative Optimization and Parametric Models}
\author{SHU Yuting}
\email{e11931687@student.tuwien.ac.at}
\affiliation{Mat.Nr. 11931687}
\author{Helmuth BREITENFELLNER}
\email{helmuth.breitenfellner@student.tuwien.ac.at}
\affiliation{Mat.Nr. 08725866}
\begin{abstract}
In this assignment we have been experimenting with various optimizers,
to better understand their behavior for stochastic gradient
descent.
In addition we were developing a first deep neural network, using CNN,
for classifying cats and dogs from images.
Finally, to reduce issues from overfitting, we applied data augmentation,
regularization, early stopping and transfer learning, in various settings.
\end{abstract}
\keywords{Deep Learning, Linear Model, Visual Computing, Image Processing, PyTorch}
\maketitle
\section{Task description}
First we were asked to experiment with various functions and stochastic
gradient descent, in combination with different optimizers and
their hyperparameters.

Next task was to create a simple CNN, based on the framework already developed
for assignment 1.

Finally, we were requested to experiment with various techniques for avoiding overfitting.

\section{Gradient Descent}

\section{Network Architecture}

\section{Avoiding of Overfitting}

\subsection{Data Augmentation}

\subsection{Regularization}

\subsection{Early Stopping}

\section{Transfer Learning}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}
\end{document}
